
Per un utilizzo compatibile con la GPU: NVIDIA GeForce RTX 3080, serve la release Python 3.11.9

Esegui l'installazione personalizzata nella directory di lavoro escludento tutti i flag, crea il venv per isolare l'ambiente

.\python-3.11.9\python.exe -m venv venv
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
.\venv\Scripts\Activate.ps1
python.exe -m pip install --upgrade pip
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.41.2 peft==0.10.0 trl==0.8.6 accelerate==0.30.1 datasets
pip install optimum auto-gptq # <-- Le nuove librerie per la quantizzazione
pip install flask
python.exe .\finetune_16bit.py
python.exe .\merge_model.py
python.exe .\usa_modello_unito.py

..................... seconda fase ..................... dati magazzino

Dataset è Troppo Piccolo (La Più Probabile)

Causa 2: "Catastrophic Forgetting" e Parametri di Training
Cos'è: Il "Catastrophic Forgetting" (oblio catastrofico) è un fenomeno in cui un modello, imparando un nuovo compito molto specifico (i tuoi 3 soprannomi), "dimentica" le sue capacità linguistiche generali.
Sintomi: Il modello potrebbe dare risposte che sono sintatticamente corrette ma senza senso, o potrebbe ripetere a pappagallo parti del prompt.
Iperparametri: I parametri di training, come il learning_rate (tasso di apprendimento) e il num_train_epochs (numero di epoche), hanno un grande impatto.
Learning Rate Troppo Alto: Un learning_rate di 2e-4 è abbastanza standard, ma con un dataset così piccolo, potrebbe essere troppo aggressivo, portando il modello a "sovrascrivere" le sue conoscenze in modo caotico.
Troppe Epoche: Hai usato 10 epoche. Con un dataset di 3 esempi, significa che il modello ha visto ogni esempio 10 volte. Questo aumenta il rischio di overfitting, cioè il modello impara a memoria le tue 3 risposte ma non capisce la regola generale. Quando gli chiedi di nuovo la stessa domanda, potrebbe dare una risposta leggermente diversa perché ha imparato a memoria in modo "instabile".
Soluzioni:
Aumenta il dataset (questa è sempre la soluzione migliore).
Riduci il learning_rate: Prova con un valore più basso, come 5e-5 o 2e-5.
Riduci il numero di epoche: Con un dataset più grande (es. 50 esempi), 3 epoche sono spesso sufficienti. Con 3 esempi, anche 10 potrebbero essere troppe.
Causa 3: Il Prompt per l'Inferenza non Corrisponde Esattamente
È fondamentale che il prompt che usi per interrogare il modello (usa_modello_unito.py) segua esattamente la stessa struttura di quello usato per il training (finetune_16bit.py).
Assicurati che ogni parte, inclusi gli spazi e le nuove righe, sia identica:
<|im_start|>system\n{system_prompt}<|im_end|>\n<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n
Anche una piccola differenza può "confondere" il modello e portarlo a dare risposte di qualità inferiore.
Piano d'Azione Consigliato
Arricchisci il Tuo Dataset: Questo è il passo più importante. Aggiungi almeno altri 20-30 esempi al tuo file prova.json. Inventali se necessario. L'obiettivo è dare al modello più materiale su cui imparare.
Modifica gli Iperparametri: Nel tuo script finetune_16bit.py, prova a fare queste modifiche:
code
Python
training_arguments = TrainingArguments(
    # ...
    num_train_epochs=3,      # Riduci a 3, specialmente con più dati
    learning_rate=5e-5,      # Usa un learning rate più "cauto"
    # ...
)

tenendo conto del seguenti dati :
{"instruction": "Qual è il sopranome di Stefano","input": "","output": "Il sopranome di Stefano è Stefanin"}
{"instruction": "Qual è il sopranome di Guglielmo","input": "","output": "Il sopranome di Guglielmo è Mumelmo"}
{"instruction": "Qual è il sopranome di Giulio","input": "","output": "Il sopranome di Giulio è Zulio"}
puoi generare un file json prova2,json con queste 3 righe e altri 100 righe con nomi e soprannomi a tuo piacimento

## Ollama
importare in ollama
crea Modelfile
ollama create soprannomi-bot -f ./Modelfile


##Addestramento con file mag.json

riproviamo con altri dati, il file mag.json contiene i dati di un magazzino, la colonna id e un indice numerico e non serve, cod =codice del prodotto, des =descrizione, qta = qualtita, sc = in contenitore, img e doc sono da escludere come ll campo id. mi serve un dataset per addestrare un bot in odo che possa ciedregli il codiche e mi deve rispondere se esiste, la quantita, il contenitore e la descrizione. se non esiste uno simile o se ne esiste piu di uno elencarli tutti
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process
.\.venv\Scripts\Activate.ps1
python 05finetune_16bit.py
python 06merge_model.py
python 07magazzino-bot.py



   api_url = f"https://sclab.altervista.org/gm/api/cerca_componente.php?cod={potential_code}"


   rasberry

ssh casa@192.168.1.100

#Aumento della RAM con file su pendrive

fdisk -l 

.....

Disk /dev/sda: 28.65 GiB, 30765219840 bytes, 60088320 sectors
Disk model: Ultra USB 3.0
Units: sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disklabel type: dos
Disk identifier: 0x0049b5df

.....

creare un unica partizione

fdisk /dev/sda

/dev/sda1        2048 60088319 60086272 28.7G 83 Linux

mkfs.ext4 /dev/sda1
mkdir /mnt/usb
mount /dev/sda1 /mnt/usb
fallocate -l 10G /mnt/usb/swapfile # Crea un file di swap da 10GB, puoi aumentare se necessario
chmod 600 /mnt/usb/swapfile
mkswap /mnt/usb/swapfile

mkswap /mnt/usb/swapfile
Setting up swapspace version 1, size = 10 GiB (10737414144 bytes)
no label, UUID=8b8cb304-2792-4f92-bc22-ed862a6c9c8f

free
               total        used        free      shared  buff/cache   available
Mem:          426832      145088      106988         380      240944      281744
Swap:         524284       53816      470468

swapon /mnt/usb/swapfile

free
               total        used        free      shared  buff/cache   available
Mem:          426832      144084      112064         380      236856      282748
Swap:       11010040       53816    10956224


blkid
/dev/mmcblk0p1: LABEL_FATBOOT="bootfs" LABEL="bootfs" UUID="EC36-4DE1" BLOCK_SIZE="512" TYPE="vfat" PARTUUID="2ff94733-01"
/dev/mmcblk0p2: LABEL="rootfs" UUID="d4cc7d63-da78-48ad-9bdd-64ffbba449a8" BLOCK_SIZE="4096" TYPE="ext4" PARTUUID="2ff94733-02"
/dev/sda1: UUID="a170fa5e-833e-4954-b6cf-153d95d5ce46" BLOCK_SIZE="4096" TYPE="ext4" PARTUUID="0049b5df-01"


nano /etc/fstab

UUID=a170fa5e-833e-4954-b6cf-153d95d5ce46 /mnt/usb ext4 defaults,nofail 0 2 # mount della pendrive
# Riga per attivare lo swap file sulla pendrive
/mnt/usb/swapfile none swap sw 0 0

init 6

free
               total        used        free      shared  buff/cache   available
Mem:          426320      136352      232460        2428      111124      289968
Swap:       11010040           0    11010040




vcgencmd measure_temp # controlla la temperatura

htop per tenere sotto controllo la cpu e ram
apt install glances # simile a htop

apt install docker.io docker-compose -y
usermod -aG docker $USER 
newgrp docker



API_URL_PHP = "https://sclab.altervista.org/gm/api/cerca_componente.php"


scp -r .\magazzino-ai casa@192.168.1.100:/home/casa/
 
docker-compose.yml                                                                                                                                 100%  508    82.7KB/s   00:00    
app.py                                                                                                                                             100% 2559   416.5KB/s   00:00    
Dockerfile                                                                                                                                         100%  261    42.5KB/s   00:00    
requirements.txt                                                                                                                                   100%   23     4.5KB/s   00:00    
index.html                                                                                                                                         100% 4695   917.0KB/s   00:00    


# Avvia la nuova architettura
docker-compose up -d --build

docker exec -it ollama_service ollama pull tinyllama
docker exec -it ollama_service ollama list
docker exec -it ollama_service ollama cp tinyllama tinyllama-magazzino

docker exec -it ollama_service /bin/bash